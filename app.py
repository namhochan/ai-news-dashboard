# -*- coding: utf-8 -*-
"""
AI ë‰´ìŠ¤ë¦¬í¬íŠ¸ â€“ ì™„ì „ í†µí•© í’€ë²„ì „ (2025-10-23)
ì§€ìˆ˜Â·ë‰´ìŠ¤Â·í…Œë§ˆÂ·ì˜ˆì¸¡ê¹Œì§€ ìë™ ì—…ë°ì´íŠ¸
"""

import math, re, numpy as np, pandas as pd
from datetime import datetime, timedelta, date
from zoneinfo import ZoneInfo
from urllib.parse import quote_plus
from collections import Counter

import streamlit as st
import yfinance as yf
import feedparser
from bs4 import BeautifulSoup
from sklearn.linear_model import LogisticRegression

KST = ZoneInfo("Asia/Seoul")

# =========================================
# ğŸ§  ê¸°ë³¸ ì„¤ì •
# =========================================
st.set_page_config(page_title="AI ë‰´ìŠ¤ë¦¬í¬íŠ¸ â€“ ì™„ì „ í†µí•©ë²„ì „", layout="wide")

def fmt_number(v, d=2):
    try: return f"{v:,.{d}f}" if v is not None and not math.isnan(v) else "-"
    except: return "-"

def fmt_percent(v):
    try: return f"{v:+.2f}%" if v is not None and not math.isnan(v) else "-"
    except: return "-"

# =========================================
# ğŸ“ˆ ì‹œì„¸ ìˆ˜ì§‘
# =========================================
def fetch_quote(ticker: str):
    try:
        t = yf.Ticker(ticker)
        l, p = getattr(t.fast_info, "last_price", None), getattr(t.fast_info, "previous_close", None)
        if l and p: return float(l), float(p)
    except: pass
    try:
        df = yf.download(ticker, period="7d", interval="1d", progress=False)
        c = df["Close"].dropna()
        if len(c)==0: return None,None
        return float(c.iloc[-1]), float(c.iloc[-2]) if len(c)>1 else None
    except: return None,None

# =========================================
# ğŸ“° ë‰´ìŠ¤ ìˆ˜ì§‘ (Google RSS)
# =========================================
def clean_html(raw): return BeautifulSoup(raw or "", "html.parser").get_text(" ", strip=True)
def _parse_entries(feed, days):
    now=datetime.now(KST); out=[]
    for e in feed.entries:
        t=None
        if getattr(e,"published_parsed",None):
            t=datetime(*e.published_parsed[:6],tzinfo=KST)
        if t and (now-t).days>days: continue
        title,link=e.get("title",""),e.get("link","")
        if link.startswith("./"): link="https://news.google.com/"+link[2:]
        out.append({"title":title,"link":link,"time":t.strftime("%Y-%m-%d %H:%M") if t else "-", "desc":clean_html(e.get("summary",""))})
    return out

def fetch_google_news_by_keyword(kw,days=3):
    q=quote_plus(kw)
    url=f"https://news.google.com/rss/search?q={q}&hl=ko&gl=KR&ceid=KR%3Ako"
    feed=feedparser.parse(url,request_headers={"User-Agent":"Mozilla/5.0"})
    return _parse_entries(feed,days)

CATEGORIES={
    "ê²½ì œë‰´ìŠ¤":["ê²½ì œ","ê¸ˆë¦¬","í™˜ìœ¨","ë¬´ì—­","ì„±ì¥ë¥ "],
    "ì£¼ì‹ë‰´ìŠ¤":["ì½”ìŠ¤í”¼","ì½”ìŠ¤ë‹¥","ì¦ì‹œ","ì£¼ê°€","ì™¸êµ­ì¸","ê¸°ê´€ë§¤ìˆ˜"],
    "ì‚°ì—…ë‰´ìŠ¤":["ë°˜ë„ì²´","AI","ë¡œë´‡","ìë™ì°¨","ë°°í„°ë¦¬","ìˆ˜ì¶œì…"],
    "ì •ì±…ë‰´ìŠ¤":["ì •ì±…","ì •ë¶€","ì˜ˆì‚°","ì„¸ê¸ˆ","ê·œì œ","ì‚°ì—…ë¶€"],
}

def fetch_category_news(cat,days=3,max_items=100):
    seen=set();out=[]
    for kw in CATEGORIES.get(cat,[]):
        for it in fetch_google_news_by_keyword(kw,days):
            k=(it["title"],it["link"])
            if k in seen:continue
            seen.add(k);out.append(it)
    out.sort(key=lambda x:x["time"],reverse=True)
    return out[:max_items]

# =========================================
# ğŸ’¹ í‹°ì»¤ë°”
# =========================================
def build_ticker_items():
    rows=[("KOSPI","^KS11",2),("KOSDAQ","^KQ11",2),
          ("DOW","^DJI",2),("NASDAQ","^IXIC",2),
          ("USD/KRW","KRW=X",2),("WTI","CL=F",2),
          ("Gold","GC=F",2),("Copper","HG=F",3)]
    items=[]
    for name,ticker,dp in rows:
        l,p=fetch_quote(ticker)
        d=(l-p) if (l and p) else 0
        pct=(d/p*100) if (l and p) else 0
        items.append({"name":name,"last":fmt_number(l,dp),"pct":fmt_percent(pct),
                      "is_up":d>0,"is_down":d<0})
    return items

TICKER_CSS="""
<style>
.ticker-wrap{overflow:hidden;width:100%;border:1px solid #263042;border-radius:10px;background:#0f1420;}
.ticker-track{display:flex;gap:16px;align-items:center;width:max-content;
will-change:transform;animation:ticker-scroll var(--speed,30s) linear infinite;}
@keyframes ticker-scroll{0%{transform:translateX(0);}100%{transform:translateX(-50%);}}
.badge{display:inline-flex;align-items:center;gap:8px;background:#0f1420;
border:1px solid #2b3a55;color:#c7d2fe;padding:6px 10px;border-radius:8px;font-weight:700;white-space:nowrap;}
.badge .name{color:#9fb3c8;font-weight:600;}
.badge .up{color:#e66;} .badge .down{color:#6aa2ff;}
</style>
"""
st.markdown(TICKER_CSS,unsafe_allow_html=True)
def render_ticker(items):
    chips=[]
    for it in items:
        arrow="â–²" if it["is_up"] else ("â–¼" if it["is_down"] else "â€¢")
        cls="up" if it["is_up"] else ("down" if it["is_down"] else "")
        chips.append(f"<span class='badge'><span class='name'>{it['name']}</span>{it['last']} <span class='{cls}'>{arrow} {it['pct']}</span></span>")
    line=" ".join(chips)
    st.markdown(f"<div class='ticker-wrap'><div class='ticker-track'>{line} {line}</div></div>",unsafe_allow_html=True)

st.markdown("## ğŸ§  AI ë‰´ìŠ¤ë¦¬í¬íŠ¸ â€“ ì‹¤ì‹œê°„ ì§€ìˆ˜ í‹°ì»¤ë°”")
render_ticker(build_ticker_items())
st.caption("ìƒìŠ¹=ë¹¨ê°•, í•˜ë½=íŒŒë‘")

# =========================================
# ğŸ“° ìµœì‹  ë‰´ìŠ¤
# =========================================
st.divider()
st.markdown("## ğŸ“° ìµœì‹  ë‰´ìŠ¤ ìš”ì•½")
cat=st.selectbox("ì¹´í…Œê³ ë¦¬ ì„ íƒ",list(CATEGORIES))
page=st.number_input("í˜ì´ì§€",1,10,1)
news_all=fetch_category_news(cat,3,100)
start=(page-1)*10
for n in news_all[start:start+10]:
    st.markdown(f"**[{n['title']}]({n['link']})**  \n<small>{n['time']}</small><br>{n['desc']}",unsafe_allow_html=True)
st.caption(f"ìµœê·¼ 3ì¼ê°„ ë‰´ìŠ¤ {len(news_all)}ê±´ ì¤‘ {start+1}~{min(start+10,len(news_all))}")

# ==== ë‰´ìŠ¤ ê¸°ë°˜ í…Œë§ˆ ìš”ì•½ ì¤€ë¹„ (ê¹¨ë—í•œ ë²„ì „) ====
st.divider()
st.markdown("## ğŸ”¥ ë‰´ìŠ¤ ê¸°ë°˜ í…Œë§ˆ ìš”ì•½")

THEME_KEYWORDS = {
    "AI":        ["ai", "ì¸ê³µì§€ëŠ¥", "ìƒì„±í˜•", "ì±—ë´‡", "ì˜¤í”ˆai", "ì—”ë¹„ë””ì•„", "gpu"],
    "ë°˜ë„ì²´":     ["ë°˜ë„ì²´", "hbm", "ë©”ëª¨ë¦¬", "íŒŒìš´ë“œë¦¬", "ì¹©", "ë¨", "ì†Œë¶€ì¥"],
    "ë¡œë´‡":       ["ë¡œë´‡", "ììœ¨ì£¼í–‰ë¡œë´‡", "amr", "í˜‘ë™ë¡œë´‡", "ë¡œë³´í‹±ìŠ¤"],
    "ì´ì°¨ì „ì§€":    ["2ì°¨ì „ì§€", "ì´ì°¨ì „ì§€", "ë°°í„°ë¦¬", "ì „ê³ ì²´", "ì–‘ê·¹ì¬", "ìŒê·¹ì¬", "lfp"],
    "ì—ë„ˆì§€":     ["ì—ë„ˆì§€", "ìœ ê°€", "ì „ë ¥", "ê°€ìŠ¤", "ì •ìœ ", "ì¬ìƒì—ë„ˆì§€", "í’ë ¥", "íƒœì–‘ê´‘"],
    "ì¡°ì„ ":       ["ì¡°ì„ ", "ì„ ë°•", "ìˆ˜ì£¼", "lngì„ ", "í•´ìš´"],
    "ì›ì „":       ["ì›ì „", "ì›ìë ¥", "smr", "ì›ì „ìˆ˜ì¶œ", "ì›ì „ì •ë¹„"],
    "ë°”ì´ì˜¤":     ["ë°”ì´ì˜¤", "ì œì•½", "ì‹ ì•½", "ì„ìƒ", "í•­ì•”", "ë°”ì´ì˜¤ì‹œë°€ëŸ¬"],
}

THEME_STOCKS = {
    "AI":       [("ì‚¼ì„±ì „ì","005930.KS"), ("ë„¤ì´ë²„","035420.KS"), ("ì•Œì²´ë¼","347860.KQ"), ("ì†”íŠ¸ë£©ìŠ¤","304100.KQ")],
    "ë°˜ë„ì²´":   [("SKí•˜ì´ë‹‰ìŠ¤","000660.KS"), ("DBí•˜ì´í…","000990.KS"), ("ë¦¬ë…¸ê³µì—…","058470.KQ"), ("í•œë¯¸ë°˜ë„ì²´","042700.KQ")],
    "ë¡œë´‡":     [("ë ˆì¸ë³´ìš°ë¡œë³´í‹±ìŠ¤","277810.KQ"), ("ìœ ì§„ë¡œë´‡","056080.KQ"), ("í‹°ë¡œë³´í‹±ìŠ¤","117730.KQ"), ("ë¡œë³´ìŠ¤íƒ€","090360.KQ")],
    "ì´ì°¨ì „ì§€": [("ì—ì½”í”„ë¡œ","086520.KQ"), ("ì—ì½”í”„ë¡œë¹„ì— ","247540.KQ"), ("ì—˜ì•¤ì—í”„","066970.KQ"), ("í¬ìŠ¤ì½”í“¨ì²˜ì— ","003670.KS")],
    "ì—ë„ˆì§€":   [("í•œêµ­ì „ë ¥","015760.KS"), ("ë‘ì‚°ì—ë„ˆë¹Œë¦¬í‹°","034020.KS"), ("í•œí™”ì†”ë£¨ì…˜","009830.KS"), ("OCIí™€ë”©ìŠ¤","010060.KS")],
    "ì¡°ì„ ":     [("HDí•œêµ­ì¡°ì„ í•´ì–‘","009540.KS"), ("HDí˜„ëŒ€ë¯¸í¬","010620.KS"), ("í•œí™”ì˜¤ì…˜","042660.KS"), ("ì‚¼ì„±ì¤‘ê³µì—…","010140.KS")],
    "ì›ì „":     [("ë‘ì‚°ì—ë„ˆë¹Œë¦¬í‹°","034020.KS"), ("ìš°ì§„","105840.KQ"), ("í•œì „KPS","051600.KS"), ("ë³´ì„±íŒŒì›Œí…","006910.KQ")],
    "ë°”ì´ì˜¤":   [("ì…€íŠ¸ë¦¬ì˜¨","068270.KS"), ("ì—ìŠ¤í‹°íŒœ","237690.KQ"), ("ì•Œí…Œì˜¤ì  ","196170.KQ"), ("ë©”ë””í†¡ìŠ¤","086900.KQ")],
}

def _normalize(s: str) -> str:
    return (s or "").lower()

def _detect_themes(news_list):
    counts = {t: 0 for t in THEME_KEYWORDS}
    sample_link = {t: "" for t in THEME_KEYWORDS}
    for n in news_list:
        text = _normalize(f"{n.get('title','')} {n.get('desc','')}")
        for theme, kws in THEME_KEYWORDS.items():
            if any(k in text for k in kws):
                counts[theme] += 1
                if not sample_link[theme]:
                    sample_link[theme] = n.get("link","")
    rows = []
    for t, c in counts.items():
        if c > 0:
            rows.append({
                "theme": t,
                "count": c,
                "sample_link": sample_link[t],
                "rep_stocks": " Â· ".join([nm for nm, _ in THEME_STOCKS.get(t, [])]) or "-",
            })
    rows.sort(key=lambda x: x["count"], reverse=True)
    return rows

# ğŸ”§ ì—¬ê¸°ì„œ â€˜NULL 4ê°œâ€™ê°€ ëœ¨ë˜ ì›ì¸ ì œê±°: ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ ì¶œë ¥ X, ëª…ì‹œì  for ë£¨í”„ ì‚¬ìš©
all_news = []
for cat_name in CATEGORIES.keys():
    all_news.extend(fetch_category_news(cat_name, days=3, max_items=100))

theme_rows = _detect_themes(all_news)

# (ì´ ì•„ë˜ë¡œëŠ” ê¸°ì¡´ í‘œì‹œ ë¡œì§ ê·¸ëŒ€ë¡œ ì‚¬ìš©)

# =========================================
# ğŸ§  ë‰´ìŠ¤ ìš”ì•½ì—”ì§„ (ë”ë³´ê¸°)
# =========================================
st.divider(); st.markdown("## ğŸ§  ë‰´ìŠ¤ ìš”ì•½ì—”ì§„")
titles=[n["title"] for n in all_news]
words=[]; [words.extend(re.sub(r"[^ê°€-í£A-Za-z0-9 ]"," ",t).split()) for t in titles]
kw=[w for w,_ in Counter([w for w in words if len(w)>=2]).most_common(10)]
st.markdown("**í‚¤ì›Œë“œ TOP10:** "+", ".join(kw))
summary=[s for s in re.split(r'[.!?]\s+', " ".join(titles)) if len(s.strip())>20][:5]
if summary:
    st.markdown(f"**ìš”ì•½:** {summary[0][:120]}...")
    with st.expander("ì „ì²´ ìš”ì•½ë¬¸ ë³´ê¸° ğŸ‘‡"):
        for s in summary: st.markdown(f"- {s.strip()}")

# =========================================
# ğŸ“Š AI ìƒìŠ¹í™•ë¥  ë¦¬í¬íŠ¸ + ìœ ë§ì¢…ëª© Top5 + ì˜ˆì¸¡
# =========================================
st.divider(); st.markdown("## ğŸ“Š AI ìƒìŠ¹í™•ë¥  ë¦¬í¬íŠ¸")

def avg_delta(stocks):
    arr=[]
    for _,t in stocks:
        l,p=fetch_quote(t)
        if l and p: arr.append((l-p)/p*100)
    return np.mean(arr) if arr else 0

rep=[]
for tr in theme_rows[:5]:
    theme=tr["theme"]; avg=avg_delta(THEME_STOCKS.get(theme,[]))
    level=1 if avg>2 else (2 if avg>0 else 3 if avg>-2 else 4)
    rep.append({"í…Œë§ˆ":theme,"ë‰´ìŠ¤ë¹ˆë„":tr["count"],"í‰ê· ë“±ë½(%)":round(avg,2),"ë¦¬ìŠ¤í¬ë ˆë²¨":level})
df=pd.DataFrame(rep); st.dataframe(df,use_container_width=True,hide_index=True)

# ğŸš€ ìœ ë§ì¢…ëª© Top5
st.divider(); st.markdown("## ğŸš€ AI ìœ ë§ ì¢…ëª© Top5")
cand=[]
for tr in theme_rows[:8]:
    for n,t in THEME_STOCKS.get(tr["theme"],[]):
        l,p=fetch_quote(t)
        if l and p:
            d=(l-p)/p*100; score=tr["count"]*0.3+d*0.7
            cand.append({"í…Œë§ˆ":tr["theme"],"ì¢…ëª©ëª…":n,"ë“±ë½ë¥ ":round(d,2),"AIì ìˆ˜":round(score,2),"í‹°ì»¤":t})
top5=pd.DataFrame(cand).sort_values("AIì ìˆ˜",ascending=False).head(5)
st.dataframe(top5,use_container_width=True,hide_index=True)

# ğŸ”® ë‚´ì¼ ì˜¤ë¥¼ í™•ë¥  (ë¡œì§€ìŠ¤í‹± íšŒê·€)
st.divider(); st.markdown("## ğŸ”® 3ì¼ ì˜ˆì¸¡ëª¨ë“ˆ")

def rsi(s,period=14):
    d=s.diff(); up=np.where(d>0,d,0); down=np.where(d<0,-d,0)
    roll_up=pd.Series(up).rolling(period).mean()
    roll_down=pd.Series(down).rolling(period).mean()
    rs=roll_up/roll_down.replace(0,np.nan)
    return 100-(100/(1+rs))

def build_feat(df):
    p=df["Close"]; f=pd.DataFrame(index=p.index)
    f["ret1"]=p.pct_change(1); f["rsi"]=rsi(p)
    f["ma5gap"]=(p-p.rolling(5).mean())/p.rolling(5).mean()
    f["ma20gap"]=(p-p.rolling(20).mean())/p.rolling(20).mean()
    f["y"]=(p.shift(-1)>p).astype(int)
    return f.dropna()

rows=[]
for _,r in top5.iterrows():
    try:
        dfh=yf.download(r["í‹°ì»¤"],period="1y",interval="1d",progress=False)
        f=build_feat(dfh)
        X=f.drop("y",axis=1).values; y=f["y"].values
        if len(X)<60: continue
        model=LogisticRegression(max_iter=200).fit(X[:-3],y[:-3])
        prob=model.predict_proba(X[-3:])[:,1]
        rows.append({"ì¢…ëª©ëª…":r["ì¢…ëª©ëª…"],"í‹°ì»¤":r["í‹°ì»¤"],
                     "ë‚´ì¼ìƒìŠ¹í™•ë¥ ":round(prob[0]*100,1),"3ì¼í‰ê· ":round(prob.mean()*100,1)})
    except: continue

if rows:
    st.dataframe(pd.DataFrame(rows),use_container_width=True,hide_index=True)
else:
    st.info("ì˜ˆì¸¡ ë°ì´í„° ë¶€ì¡±")

