# app.py â€” pandas_datareader ì—†ì´ ë™ìž‘í•˜ëŠ” ë²„ì „
import os, json, re
from datetime import datetime, timezone, timedelta
from typing import Any, Dict, List, Tuple, Optional

import streamlit as st
import pandas as pd
import yfinance as yf
import feedparser

# ---------------- Common ----------------
KST = timezone(timedelta(hours=9))
st.set_page_config(page_title="AI ë‰´ìŠ¤ë¦¬í¬íŠ¸ ì¢…í•© ëŒ€ì‹œë³´ë“œ", layout="wide")
st.markdown("# ðŸ§  AI ë‰´ìŠ¤ë¦¬í¬íŠ¸ ì¢…í•© ëŒ€ì‹œë³´ë“œ (ìžë™ ì—…ë°ì´íŠ¸)")
st.caption("ì—…ë°ì´íŠ¸ ì‹œê°„: " + datetime.now(KST).strftime("%Y-%m-%d %H:%M:%S (KST)"))

def kst_now_iso(): return datetime.now(KST).isoformat()

def load_json(path: str, default: Any):
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return default

def to_f(x):
    try:
        return None if x is None else float(x)
    except:
        return None

# ---------------- Market (yfinance only) ----------------
FALLBACK = {
    "KOSPI":  ["^KS11"],
    "KOSDAQ": ["^KQ11","^KOSDAQ","KQ11"],
    "USDKRW": ["KRW=X"],
    "WTI":    ["CL=F"],
    "Gold":   ["GC=F"],
    "Copper": ["HG=F"],
}

def _yf_last2(tick: str)->Tuple[Optional[float],Optional[float]]:
    try:
        df = yf.download(tick, period="10d", interval="1d", progress=False)
        c = df.get("Close")
        if c is None: return None, None
        vals = c.dropna().tail(2).tolist()
        if len(vals)==1: return float(vals[0]), None
        if len(vals)>=2: return float(vals[-1]), float(vals[-2])
    except:
        pass
    return None, None

def last2_any(candidates: List[str])->Tuple[Optional[float],Optional[float],Optional[str]]:
    for t in candidates:
        cur, prev = _yf_last2(t)
        if cur is not None:
            return cur, prev, t
    return None, None, None

def pct(cur, prev):
    try:
        if cur is None or prev in (None, 0): return None
        return round((cur-prev)/prev*100,2)
    except:
        return None

def load_market()->Dict[str,Any]:
    data = load_json("data/market_today.json", {})
    updated=False
    for name, cands in FALLBACK.items():
        cur = to_f(data.get(name,{}).get("value"))
        asof = data.get(name,{}).get("asof")
        stale = True
        try:
            if asof:
                stale = (datetime.now(KST)-datetime.fromisoformat(asof)).total_seconds()>6*3600
        except:
            pass
        if cur is None or stale:
            v, p, used = last2_any(cands)
            data[name] = {
                "value": None if v is None else round(v,2),
                "prev": None if p is None else round(p,2),
                "change_pct": pct(v,p),
                "ticker": used,
                "asof": kst_now_iso()
            }
            updated=True
    if updated:
        try:
            os.makedirs("data", exist_ok=True)
            with open("data/market_today.json","w",encoding="utf-8") as f:
                json.dump(data,f,ensure_ascii=False,indent=2)
        except:
            pass
    return data

# ---------------- News via Google RSS ----------------
NEWS_QUERIES = [
    "site:mk.co.kr ê²½ì œ", "site:hankyung.com ê²½ì œ", "site:biz.chosun.com ì‚°ì—…",
    "site:news1.kr ì •ì±…", "site:yna.co.kr ë¦¬í¬íŠ¸", "site:policy.go.kr ì •ì±…ë¸Œë¦¬í•‘"
]

def google_rss(query, hl="ko", gl="KR", ceid="KR:ko"):
    from urllib.parse import quote
    return f"https://news.google.com/rss/search?q={quote(query)}&hl={hl}&gl={gl}&ceid={ceid}"

def fetch_headlines_top10()->List[Dict[str,str]]:
    items=[]
    for q in NEWS_QUERIES:
        url=google_rss(q)
        try:
            feed = feedparser.parse(url)
            for e in feed.entries[:5]:
                title = e.get("title","").strip()
                link  = e.get("link","").strip()
                if title and link:
                    items.append({"title":title,"link":link})
        except:
            pass
    # de-dup
    seen=set(); uniq=[]
    for it in items:
        if it["title"] in seen: continue
        seen.add(it["title"]); uniq.append(it)
    return uniq[:10]

# ---------------- Theme scoring ----------------
THEME_KEYWORDS = {
    "AI": ["AI","ì¸ê³µì§€ëŠ¥","ìƒì„±í˜•","ì±—GPT","LLM"],
    "ë°˜ë„ì²´": ["ë°˜ë„ì²´","HBM","íŒŒìš´ë“œë¦¬","ë©”ëª¨ë¦¬","GPU","ì¹©"],
    "ë¡œë´‡": ["ë¡œë´‡","í˜‘ë™ë¡œë´‡","ìžìœ¨ì£¼í–‰ë¡œë´‡"],
    "ì´ì°¨ì „ì§€": ["ì´ì°¨ì „ì§€","ë°°í„°ë¦¬","ì–‘ê·¹ìž¬","ìŒê·¹ìž¬","ì „ê³ ì²´"],
    "ë°”ì´ì˜¤": ["ë°”ì´ì˜¤","ì œì•½","ì˜ì•½í’ˆ","ìž„ìƒ"],
    "ì¡°ì„ ": ["ì¡°ì„ ","ì„ ë°•","í•´ìš´","LNGì„ "],
    "ì›ì „": ["ì›ì „","SMR","ì›ìžë ¥"],
    "ì—ë„ˆì§€": ["ì „ë ¥","ì •ìœ ","ê°€ìŠ¤","ìž¬ìƒì—ë„ˆì§€","í’ë ¥","íƒœì–‘ê´‘"],
}
THEME_STOCKS = {
    "AI": ["ì‚¼ì„±ì „ìž","ë„¤ì´ë²„","ì¹´ì¹´ì˜¤","ë”ì¡´ë¹„ì¦ˆì˜¨","í‹°ë§¥ìŠ¤ì†Œí”„íŠ¸"],
    "ë°˜ë„ì²´": ["ì‚¼ì„±ì „ìž","SKí•˜ì´ë‹‰ìŠ¤","DBí•˜ì´í…","í•œë¯¸ë°˜ë„ì²´","í…ŒìŠ¤"],
    "ë¡œë´‡": ["ë ˆì¸ë³´ìš°ë¡œë³´í‹±ìŠ¤","ìœ ì§„ë¡œë´‡","í‹°ë¡œë³´í‹±ìŠ¤","ë¡œë³´ìŠ¤íƒ€","í˜„ëŒ€ë¡œë³´í‹±ìŠ¤"],
    "ì´ì°¨ì „ì§€": ["LGì—ë„ˆì§€ì†”ë£¨ì…˜","í¬ìŠ¤ì½”í“¨ì²˜ì— ","ì—ì½”í”„ë¡œ","ì—ì½”í”„ë¡œë¹„ì— ","ì—˜ì•¤ì—í”„"],
    "ë°”ì´ì˜¤": ["ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤","ì…€íŠ¸ë¦¬ì˜¨","HLB","ì—ìŠ¤í‹°íŒœ","ë©”ë””í†¡ìŠ¤"],
    "ì¡°ì„ ": ["HDí•œêµ­ì¡°ì„ í•´ì–‘","HDí˜„ëŒ€ë¯¸í¬","ì‚¼ì„±ì¤‘ê³µì—…","í•œí™”ì˜¤ì…˜","HSDì—”ì§„"],
    "ì›ì „": ["ë‘ì‚°ì—ë„ˆë¹Œë¦¬í‹°","ìš°ì§„","í•œì „KPS","í•œì „ê¸°ìˆ ","ì¼ì§„íŒŒì›Œ"],
    "ì—ë„ˆì§€": ["í•œêµ­ì „ë ¥","ë‘ì‚°ì—ë„ˆë¹Œë¦¬í‹°","GS","SKì´ë…¸ë² ì´ì…˜","í•œêµ­ê°€ìŠ¤ê³µì‚¬"],
}

def tokenize_ko(text:str)->List[str]:
    text = re.sub(r"[^0-9A-Za-zê°€-íž£ ]"," ", text)
    return [t for t in text.split() if t]

def score_themes(news: List[Dict[str,str]])->pd.DataFrame:
    counts = {k:0 for k in THEME_KEYWORDS}
    sample = {k:"" for k in THEME_KEYWORDS}
    for n in news:
        title = n.get("title","")
        tokens = tokenize_ko(title)
        tset = " ".join(tokens)
        for theme, keys in THEME_KEYWORDS.items():
            if any(k in tset for k in keys):
                counts[theme]+=1
                if not sample[theme]: sample[theme]=n.get("link","#")
    rows=[]
    for th, ct in counts.items():
        if ct>0:
            rows.append({
                "theme": th,
                "count": ct,
                "score": ct,
                "rep_stocks": " Â· ".join(THEME_STOCKS.get(th, [])),
                "sample_link": sample[th]
            })
    return pd.DataFrame(rows).sort_values(["score","count"], ascending=False)

def monthly_keywords(news: List[Dict[str,str]])->pd.DataFrame:
    bag={}
    for n in news:
        for w in tokenize_ko(n.get("title","")):
            if len(w)<2: continue
            bag[w]=bag.get(w,0)+1
    rows = sorted(bag.items(), key=lambda x:x[1], reverse=True)[:30]
    return pd.DataFrame([{"keyword":k,"count":v} for k,v in rows])

# ---------------- Prepare data ----------------
market = load_market()

headlines = load_json("data/headlines_top10.json", [])
if not headlines:
    headlines = fetch_headlines_top10()
    try:
        os.makedirs("data", exist_ok=True)
        with open("data/headlines_top10.json","w",encoding="utf-8") as f:
            json.dump(headlines,f,ensure_ascii=False,indent=2)
    except:
        pass

themes_df = score_themes(headlines) if headlines else pd.DataFrame(columns=["theme","count","score","rep_stocks","sample_link"])
keywords_df = monthly_keywords(headlines) if headlines else pd.DataFrame(columns=["keyword","count"])

# ---------------- UI ----------------
def metric_block(col, title, entry: Dict[str,Any]):
    v = to_f(entry.get("value"))
    d = to_f(entry.get("change_pct"))
    if v is None: col.metric(title, value="-", delta="None")
    else: col.metric(title, value=f"{v:,.2f}", delta=("None" if d is None else f"{d:+.2f}%"))

st.subheader("ðŸ“Š ì˜¤ëŠ˜ì˜ ì‹œìž¥ ìš”ì•½")
c1,c2,c3 = st.columns(3); c4,c5,c6 = st.columns(3)
metric_block(c1, "KOSPI"        , market.get("KOSPI",{}))
metric_block(c2, "KOSDAQ"       , market.get("KOSDAQ",{}))
metric_block(c3, "í™˜ìœ¨(USD/KRW)" , market.get("USDKRW",{}))
metric_block(c4, "WTI"          , market.get("WTI",{}))
metric_block(c5, "Gold"         , market.get("Gold",{}))
metric_block(c6, "Copper"       , market.get("Copper",{}))

st.divider()
st.subheader("ðŸ“° ìµœì‹  ê²½ì œÂ·ì •ì±…Â·ì‚°ì—…Â·ë¦¬í¬íŠ¸ ë‰´ìŠ¤ TOP 10")
if headlines is None or len(headlines)==0:
    st.info("í—¤ë“œë¼ì¸ ì—†ìŒ")
else:
    for i,n in enumerate(headlines[:10], start=1):
        title = n.get("title","").replace("[","ï¼»").replace("]","ï¼½")
        link  = n.get("link","#") or "#"
        st.markdown(f"{i}. [{title}]({link})")

st.divider()
st.subheader("ðŸ”¥ ë‰´ìŠ¤ ê¸°ë°˜ TOP í…Œë§ˆ")
if themes_df is None or themes_df.empty:
    st.info("í…Œë§ˆ ë°ì´í„° ì—†ìŒ")
else:
    st.bar_chart(themes_df.set_index("theme")["count"])
    with st.expander("ì „ì²´ í…Œë§ˆ ì§‘ê³„ (ëŒ€í‘œ ì¢…ëª©/ìƒ˜í”Œë§í¬ í¬í•¨)"):
        st.dataframe(themes_df.reset_index(drop=True), use_container_width=True)

st.divider()
st.subheader("ðŸŒ ì›”ê°„ í‚¤ì›Œë“œë§µ (ìµœê·¼ 30ì¼)")
if keywords_df is None or keywords_df.empty:
    st.info("í‚¤ì›Œë“œ ì—†ìŒ")
else:
    st.bar_chart(keywords_df.set_index("keyword")["count"])

st.success("ëŒ€ì‹œë³´ë“œ ë¡œë”© ì™„ë£Œ (ë°ì´í„°ë¦¬ë” ì˜ì¡´ ì œê±° ë²„ì „)")
