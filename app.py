import math
from datetime import datetime, timedelta
from zoneinfo import ZoneInfo
from urllib.parse import quote_plus

import pandas as pd
import streamlit as st
import yfinance as yf
import feedparser
from bs4 import BeautifulSoup

# -----------------------------
# ê³µí†µ ì„¤ì •
# -----------------------------
st.set_page_config(page_title="AI ë‰´ìŠ¤ë¦¬í¬íŠ¸ â€“ ì‹¤ì‹œê°„ ì§€ìˆ˜ í‹°ì»¤ë°”", layout="wide")
KST = ZoneInfo("Asia/Seoul")

def fmt_number(val: float, decimals: int = 2) -> str:
    if val is None or (isinstance(val, float) and (math.isnan(val) or math.isinf(val))):
        return "-"
    return f"{val:,.{decimals}f}"

def fmt_percent(pct: float) -> str:
    if pct is None or (isinstance(pct, float) and (math.isnan(pct) or math.isinf(pct))):
        return "-"
    return f"{pct:+.2f}%"

# -----------------------------
# ì‹œì„¸ ê°€ì ¸ì˜¤ê¸° (yfinance ì•ˆì „ ëª¨ë“œ)
# -----------------------------
def fetch_quote(ticker: str):
    # 1) fast_info
    try:
        t = yf.Ticker(ticker)
        last = getattr(t.fast_info, "last_price", None)
        prev = getattr(t.fast_info, "previous_close", None)
        if last and prev:
            return float(last), float(prev)
    except Exception:
        pass
    # 2) ìµœê·¼ 7ì¼ ì¢…ê°€
    try:
        df = yf.download(ticker, period="7d", interval="1d", progress=False, auto_adjust=False)
        if df.empty:
            return None, None
        closes = df["Close"].dropna()
        last = float(closes.iloc[-1])
        prev = float(closes.iloc[-2]) if len(closes) >= 2 else None
        return last, prev
    except Exception:
        return None, None

# -----------------------------
# í‹°ì»¤ë°” ë Œë”ë§
# -----------------------------
def make_chip(name: str, last: float, prev: float):
    delta = None
    pct = None
    if last is not None and prev not in (None, 0):
        delta = last - prev
        pct = (delta / prev) * 100

    if delta is None:
        klass = "flat"
        delta_txt = "-"
    elif delta > 0:
        klass = "up"
        delta_txt = f"â–² {fmt_percent(pct)}"
    elif delta < 0:
        klass = "down"
        delta_txt = f"â–¼ {fmt_percent(pct)}"
    else:
        klass = "flat"
        delta_txt = "0.00%"

    return f"""
    <span class="chip {klass}">
      <b>{name}</b> {fmt_number(last,2)} <span class="delta">{delta_txt}</span>
    </span>
    """

TICKER_CSS = """
<style>
.tbar { display:flex; gap:10px; overflow-x:auto; white-space:nowrap;
        border:1px solid #2b3445; padding:10px 12px; border-radius:12px; }
.chip { border-radius:999px; padding:6px 12px; background:#0e1116; border:1px solid #2b3445; }
.chip .delta { margin-left:6px; font-weight:700; }
.up   { color:#e86d6d; }
.down { color:#4aa3ff; }
.flat { color:#9aa0a6; }
</style>
"""
st.markdown(TICKER_CSS, unsafe_allow_html=True)

st.markdown("# ğŸ§  AI ë‰´ìŠ¤ë¦¬í¬íŠ¸ â€“ ì‹¤ì‹œê°„ ì§€ìˆ˜ í‹°ì»¤ë°”")
st.caption("ì—…ë°ì´íŠ¸: " + datetime.now(KST).strftime("%Y-%m-%d %H:%M:%S (KST)"))

# ìƒë‹¨ ë²„íŠ¼(ê°•ì œ ìƒˆë¡œê³ ì¹¨)
col1, col2 = st.columns([1,6])
with col1:
    if st.button("ğŸ”„ ê°•ì œ ìƒˆë¡œê³ ì¹¨"):
        st.rerun()

st.subheader("ğŸ“ˆ ì˜¤ëŠ˜ì˜ ì‹œì¥ ìš”ì•½")

INDEXES = [
    ("KOSPI",   "^KS11"),
    ("KOSDAQ",  "^KQ11"),
    ("DOW",     "^DJI"),
    ("NASDAQ",  "^IXIC"),
    ("USD/KRW", "KRW=X"),
    ("WTI",     "CL=F"),
    ("Gold",    "GC=F"),
    ("Copper",  "HG=F"),
]

chips = []
for name, tick in INDEXES:
    last, prev = fetch_quote(tick)
    chips.append(make_chip(name, last, prev))

st.markdown(f'<div class="tbar">{" | ".join(chips)}</div>', unsafe_allow_html=True)
st.caption("â€» ìƒìŠ¹=ë¹¨ê°•, í•˜ë½=íŒŒë‘ Â· ë°ì´í„° ì†ŒìŠ¤: Stooq âœ Yahoo â†’ yfinance (10ë¶„ ì§€ì—°)")

# =============================
# ğŸ“° Google News RSS (3ì¼/ì¹´í…Œê³ ë¦¬/10ê°œì”© í˜ì´ì§€)
# =============================
st.divider()
st.markdown("## ğŸ“° ìµœì‹  ë‰´ìŠ¤ ìš”ì•½")

def clean_html(raw_html: str) -> str:
    if not raw_html:
        return ""
    return BeautifulSoup(raw_html, "html.parser").get_text(" ", strip=True)

def fetch_google_news(query: str, days: int = 3, max_items: int = 100):
    """
    Google News RSS
    - q íŒŒë¼ë¯¸í„°ë¥¼ URL ì¸ì½”ë”©(quote_plus)í•˜ì—¬ InvalidURL ë°©ì§€
    - when:3d ì¡°ê±´ í¬í•¨
    - í•œêµ­ì–´/í•œêµ­ ì§€ì—­
    """
    # q íŒŒë¼ë¯¸í„°ë§Œ ì¸ì½”ë”©!
    q = quote_plus(f"({query}) when:{days}d")
    rss_url = f"https://news.google.com/rss/search?q={q}&hl=ko&gl=KR&ceid=KR%3Ako"

    feed = feedparser.parse(rss_url)
    now = datetime.now(KST)
    items = []
    for e in feed.entries[:max_items]:
        try:
            title = e.title
            # ìƒëŒ€ê²½ë¡œë¥¼ ì ˆëŒ€ê²½ë¡œë¡œ ë³´ì •
            link = e.link
            if link.startswith("./"):
                link = "https://news.google.com/" + link[2:]
            # ë°œí–‰ ì‹œê°
            published = "-"
            if hasattr(e, "published_parsed") and e.published_parsed:
                pub_dt = datetime(*e.published_parsed[:6])
                # 3ì¼ í•„í„°
                if (now - pub_dt) > timedelta(days=days):
                    continue
                published = pub_dt.strftime("%Y-%m-%d %H:%M")
            desc = clean_html(e.get("summary", ""))
            items.append({"title": title, "link": link, "time": published, "desc": desc})
        except Exception:
            continue
    return items

CATEGORIES = {
    "ê²½ì œë‰´ìŠ¤": "ê²½ì œ OR ë¬¼ê°€ OR í™˜ìœ¨ OR ë¬´ì—­ OR ê¸ˆë¦¬ OR ì„±ì¥ë¥ ",
    "ì£¼ì‹ë‰´ìŠ¤": "ì½”ìŠ¤í”¼ OR ì½”ìŠ¤ë‹¥ OR ì¦ì‹œ OR ì£¼ê°€ OR ë§¤ìˆ˜ OR ê¸°ê´€ OR ì™¸êµ­ì¸",
    "ì‚°ì—…ë‰´ìŠ¤": "ì‚°ì—… OR ë°˜ë„ì²´ OR ë°°í„°ë¦¬ OR ë¡œë´‡ OR ì œì¡° OR ìˆ˜ì¶œì…",
    "ì •ì±…ë‰´ìŠ¤": "ì •ì±… OR ì •ë¶€ OR ì˜ˆì‚° OR ì„¸ê¸ˆ OR ê·œì œ OR ì§€ì› OR ì‚°ì—…ë¶€ OR ê¸ˆìœµìœ„",
}

# ë°ì´í„° ìˆ˜ì§‘ (ì¹´í…Œê³ ë¦¬ë³„ 100ê°œê¹Œì§€)
news_data = {}
for cat, query in CATEGORIES.items():
    try:
        news_data[cat] = fetch_google_news(query, days=3, max_items=100)
    except Exception as e:
        news_data[cat] = []
        st.warning(f"{cat} ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")

# UI: ì¹´í…Œê³ ë¦¬ + í˜ì´ì§€
left, right = st.columns([2,1])
with left:
    cat_selected = st.selectbox("ğŸ“‚ ì¹´í…Œê³ ë¦¬ ì„ íƒ", list(CATEGORIES.keys()))
with right:
    total = len(news_data.get(cat_selected, []))
    per_page = 10
    max_page = max(1, (total - 1) // per_page + 1)
    page = st.number_input("í˜ì´ì§€", min_value=1, max_value=max_page, value=1, step=1)

start = (page - 1) * per_page
end = start + per_page
subset = news_data.get(cat_selected, [])[start:end]

if not subset:
    st.info("í‘œì‹œí•  ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. (ìµœê·¼ 3ì¼ ì´ë‚´ ê²°ê³¼ ì—†ìŒ)")
else:
    for n in subset:
        st.markdown(f"#### [{n['title']}]({n['link']})")
        st.caption(f"ğŸ•’ {n['time']}")
        if n["desc"]:
            st.write(n["desc"])
        st.markdown("---")

st.caption(
    f"ğŸ“† ìµœê·¼ 3ì¼ | ì¹´í…Œê³ ë¦¬: {cat_selected} | "
    f"{len(news_data.get(cat_selected, []))}ê°œ ì¤‘ {start+1}â€“{min(end, len(news_data.get(cat_selected, [])))} í‘œì‹œ"
    )
