# -*- coding: utf-8 -*-
import math
from datetime import datetime, timedelta
from zoneinfo import ZoneInfo
from urllib.parse import quote_plus

import streamlit as st
import yfinance as yf
import feedparser
from bs4 import BeautifulSoup

KST = ZoneInfo("Asia/Seoul")

# ---------------------------------
# í˜ì´ì§€ ì„¤ì •
# ---------------------------------
st.set_page_config(
    page_title="AI ë‰´ìŠ¤ë¦¬í¬íŠ¸ â€“ ì‹¤ì‹œê°„ ì§€ìˆ˜ í‹°ì»¤ë°” + ìµœì‹  ë‰´ìŠ¤",
    layout="wide",
)

# ---------------------------------
# ê³µí†µ ìœ í‹¸
# ---------------------------------
def fmt_number(val: float, decimals: int = 2) -> str:
    try:
        if val is None or (isinstance(val, float) and (math.isnan(val) or math.isinf(val))):
            return "-"
        return f"{val:,.{decimals}f}"
    except Exception:
        return "-"

def fmt_percent(pct: float) -> str:
    try:
        if pct is None or (isinstance(pct, float) and (math.isnan(pct) or math.isinf(pct))):
            return "-"
        return f"{pct:+.2f}%"
    except Exception:
        return "-"

# ---------------------------------
# ì‹œì„¸ ìˆ˜ì§‘ (ì•ˆì •í˜•)
# ---------------------------------
def fetch_quote(ticker: str):
    """
    1) fast_info ì‹œë„
    2) ì‹¤íŒ¨ ì‹œ ìµœê·¼ 7ì¼/1ì¼ë´‰ ì¢…ê°€ë¡œ ê³„ì‚°
    """
    try:
        t = yf.Ticker(ticker)
        last = getattr(t.fast_info, "last_price", None)
        prev = getattr(t.fast_info, "previous_close", None)
        if last and prev:
            return float(last), float(prev)
    except Exception:
        pass

    try:
        df = yf.download(ticker, period="7d", interval="1d", auto_adjust=False, progress=False)
        closes = df.get("Close")
        if df is None or closes is None or closes.dropna().empty:
            return None, None
        closes = closes.dropna()
        last = float(closes.iloc[-1])
        prev = float(closes.iloc[-2]) if len(closes) >= 2 else None
        return last, prev
    except Exception:
        return None, None

def build_ticker_items():
    """
    í‹°ì»¤ë°”ì— í‘œì‹œí•  í•­ëª© êµ¬ì„±
    """
    rows = [
        ("KOSPI",   "^KS11", 2),
        ("KOSDAQ",  "^KQ11", 2),
        ("DOW",     "^DJI",  2),
        ("NASDAQ",  "^IXIC", 2),
        ("USD/KRW", "KRW=X", 2),
        ("WTI",     "CL=F",  2),
        ("Gold",    "GC=F",  2),
        ("Copper",  "HG=F",  3),
    ]
    items = []
    for (name, ticker, dp) in rows:
        last, prev = fetch_quote(ticker)
        delta = None
        pct = None
        if last is not None and prev not in (None, 0):
            delta = last - prev
            pct = (delta / prev) * 100.0
        items.append({
            "name": name,
            "last": fmt_number(last, dp),
            "pct": fmt_percent(pct) if pct is not None else "--",
            "is_up": (delta or 0) > 0,
            "is_down": (delta or 0) < 0,
        })
    return items

# ---------------------------------
# ë‰´ìŠ¤ ìˆ˜ì§‘ (Google News RSS) â€“ ì•ˆì • ë²„ì „
# ---------------------------------
def clean_html(raw_html: str) -> str:
    if not raw_html:
        return ""
    return BeautifulSoup(raw_html, "html.parser").get_text(" ", strip=True)

def _parse_entries(feed, days: int):
    now = datetime.now(KST)
    items = []
    for e in feed.entries:
        pub_dt = None
        if getattr(e, "published_parsed", None):
            pub_dt = datetime(*e.published_parsed[:6], tzinfo=KST)
        elif getattr(e, "updated_parsed", None):
            pub_dt = datetime(*e.updated_parsed[:6], tzinfo=KST)

        if pub_dt and (now - pub_dt) > timedelta(days=days):
            continue

        title = getattr(e, "title", "").strip()
        link = getattr(e, "link", "").strip()
        if link.startswith("./"):
            link = "https://news.google.com/" + link[2:]
        desc = clean_html(getattr(e, "summary", ""))

        items.append({
            "title": title,
            "link": link,
            "time": pub_dt.strftime("%Y-%m-%d %H:%M") if pub_dt else "-",
            "desc": desc
        })
    return items

def fetch_google_news_by_keyword(keyword: str, days: int = 3, max_items: int = 40):
    """
    ë‹¨ì¼ í‚¤ì›Œë“œë¥¼ ì•ˆì „í•˜ê²Œ RSS ì¡°íšŒ (User-Agent ì§€ì •)
    """
    q = quote_plus(keyword)
    url = f"https://news.google.com/rss/search?q={q}&hl=ko&gl=KR&ceid=KR%3Ako"
    feed = feedparser.parse(url, request_headers={"User-Agent": "Mozilla/5.0"})
    items = _parse_entries(feed, days)
    return items[:max_items]

# ì¹´í…Œê³ ë¦¬ â†’ ì—¬ëŸ¬ í‚¤ì›Œë“œ
CATEGORIES = {
    "ê²½ì œë‰´ìŠ¤": ["ê²½ì œ", "ë¬¼ê°€", "í™˜ìœ¨", "ë¬´ì—­", "ê¸ˆë¦¬", "ì„±ì¥ë¥ "],
    "ì£¼ì‹ë‰´ìŠ¤": ["ì½”ìŠ¤í”¼", "ì½”ìŠ¤ë‹¥", "ì¦ì‹œ", "ì£¼ê°€", "ì™¸êµ­ì¸ ë§¤ìˆ˜", "ê¸°ê´€ ë§¤ë§¤"],
    "ì‚°ì—…ë‰´ìŠ¤": ["ì‚°ì—…", "ë°˜ë„ì²´", "ë°°í„°ë¦¬", "ë¡œë´‡", "ì œì¡°", "ìˆ˜ì¶œì…"],
    "ì •ì±…ë‰´ìŠ¤": ["ì •ì±…", "ì •ë¶€", "ì˜ˆì‚°", "ì„¸ê¸ˆ", "ê·œì œ", "ì‚°ì—…ë¶€", "ê¸ˆìœµìœ„ì›íšŒ"],
}

def fetch_category_news(cat: str, days: int = 3, max_items: int = 100):
    """
    í‚¤ì›Œë“œë³„ë¡œ ì¡°íšŒí•´ì„œ í•©ì¹˜ê³ (ì¤‘ë³µ ì œê±°), ìµœì‹ ìˆœ ì •ë ¬
    """
    seen = set()
    merged = []
    for kw in CATEGORIES.get(cat, []):
        try:
            for it in fetch_google_news_by_keyword(kw, days=days, max_items=40):
                key = (it["title"], it["link"])
                if key in seen:
                    continue
                seen.add(key)
                merged.append(it)
        except Exception:
            continue

    def _key(x):
        try:
            return datetime.strptime(x["time"], "%Y-%m-%d %H:%M")
        except Exception:
            return datetime.min

    merged.sort(key=_key, reverse=True)
    return merged[:max_items]

# ---------------------------------
# UI â€“ í—¤ë” / í‹°ì»¤ë°”
# ---------------------------------
st.markdown("## ğŸ§  AI ë‰´ìŠ¤ë¦¬í¬íŠ¸ â€“ ì‹¤ì‹œê°„ ì§€ìˆ˜ í‹°ì»¤ë°”")
st.caption(f"ì—…ë°ì´íŠ¸: {datetime.now(KST).strftime('%Y-%m-%d %H:%M:%S (KST)')}")

colL, colR = st.columns([1, 5])
with colL:
    st.markdown("### ğŸ“Š ì˜¤ëŠ˜ì˜ ì‹œì¥ ìš”ì•½")
with colR:
    if st.button("ğŸ”„ ìƒˆë¡œê³ ì¹¨", use_container_width=False):
        st.cache_data.clear()
        st.rerun()

# í‹°ì»¤ ë°ì´í„°
ticker_items = build_ticker_items()

TICKER_CSS = """
<style>
.ticker-wrap {
  position: relative; overflow: hidden; width: 100%;
  border: 1px solid #263042; border-radius: 10px; background: #0f1420;
}
.ticker {
  display: inline-block; white-space: nowrap;
  padding: 8px 0; animation: scroll-left var(--speed, 35s) linear infinite;
}
@keyframes scroll-left {
  0% { transform: translateX(0); }
  100% { transform: translateX(-50%); }
}
.badge {
  display:inline-flex; align-items:center; gap:8px;
  background:#0f1420; border:1px solid #2b3a55; color:#c7d2fe;
  padding:6px 10px; margin:0 8px; border-radius:8px; font-weight:700;
}
.badge .name { color:#9fb3c8; font-weight:600; }
.badge .up   { color:#e66; }
.badge .down { color:#6aa2ff; }
.sep { color:#44526b; padding: 0 8px; }
</style>
"""
st.markdown(TICKER_CSS, unsafe_allow_html=True)

def render_ticker_line(items, speed_sec=35):
    badges = []
    for it in items:
        arrow = "â–²" if it["is_up"] else ("â–¼" if it["is_down"] else "â€¢")
        pct_class = "up" if it["is_up"] else ("down" if it["is_down"] else "")
        badges.append(
            f'<span class="badge"><span class="name">{it["name"]}</span>'
            f'{it["last"]} <span class="{pct_class}">{arrow} {it["pct"]}</span></span>'
        )
    line = '<span class="sep">|</span>'.join(badges)
    # ë‘ ë²ˆ ì´ì–´ë¶™ì—¬ ëŠê¹€ ì—†ì´
    html = f"""
    <div class="ticker-wrap" style="--speed:{speed_sec}s">
      <div class="ticker">{line} <span class="sep">|</span> {line}</div>
    </div>
    """
    st.markdown(html, unsafe_allow_html=True)

render_ticker_line(ticker_items, speed_sec=35)
st.caption("â€» ìƒìŠ¹=ë¹¨ê°•, í•˜ë½=íŒŒë‘ Â· ë°ì´í„° ì†ŒìŠ¤: Stooq â†’ Yahoo â†’ yfinance (10ë¶„ ì§€ì—°)")

st.divider()

# ---------------------------------
# UI â€“ ìµœì‹  ë‰´ìŠ¤ (ì¹´í…Œê³ ë¦¬/í˜ì´ì§€)
# ---------------------------------
st.markdown("## ğŸ“° ìµœì‹  ë‰´ìŠ¤ ìš”ì•½")

c1, c2 = st.columns([2, 1])
with c1:
    cat = st.selectbox("ğŸ“‚ ì¹´í…Œê³ ë¦¬ ì„ íƒ", list(CATEGORIES.keys()))
with c2:
    page = st.number_input("í˜ì´ì§€", min_value=1, value=1, step=1)

# ë°ì´í„° ë¡œë“œ
news_all = fetch_category_news(cat, days=3, max_items=100)
page_size = 10
total = len(news_all)
start = (page - 1) * page_size
end = start + page_size
news_page = news_all[start:end]

if not news_page:
    st.info("í‘œì‹œí•  ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. (ìµœê·¼ 3ì¼ ë‚´ ê²°ê³¼ ì—†ìŒ)")
else:
    for i, n in enumerate(news_page, start=1 + start):
        title = n.get("title", "").strip() or "(ì œëª© ì—†ìŒ)"
        link = n.get("link", "")
        when = n.get("time", "-")
        desc = n.get("desc", "")

        st.markdown(
            f"**{i}. [{title}]({link})**  \n"
            f"<span style='color:#9aa0a6;font-size:0.9rem;'>{when}</span><br>"
            f"<span style='color:#aeb8c5;'>{desc}</span>",
            unsafe_allow_html=True
        )
        st.markdown("<hr style='border:0;border-top:1px solid #1f2937'/>", unsafe_allow_html=True)

st.caption(f"ğŸ—“ ìµœê·¼ 3ì¼ Â· ì¹´í…Œê³ ë¦¬: {cat} Â· {total}ê°œ ì¤‘ {start+1}-{min(end,total)} í‘œì‹œ")

with st.expander("ğŸ§ª ë””ë²„ê·¸(ìˆ˜ì§‘ê²°ê³¼ ë° ìš”ì²­ í™•ì¸)"):
    st.write({"cat": cat, "total": total, "page": page, "start": start, "end": end})
