# app.py â€” íŒŒì¼/ì™¸ë¶€ë°ì´í„° ì—†ì–´ë„ ë°”ë¡œ ë™ìž‘í•˜ë„ë¡ ìžê¸‰ìžì¡± ë²„ì „
import os, json, re
from datetime import datetime, timezone, timedelta
from typing import Any, Dict, List, Optional, Tuple

import streamlit as st
import pandas as pd

# ---- ê³µí†µ ----
KST = timezone(timedelta(hours=9))
st.set_page_config(page_title="AI ë‰´ìŠ¤ë¦¬í¬íŠ¸ ì¢…í•© ëŒ€ì‹œë³´ë“œ", layout="wide")
st.markdown("# ðŸ§  AI ë‰´ìŠ¤ë¦¬í¬íŠ¸ ì¢…í•© ëŒ€ì‹œë³´ë“œ (ìžë™ ì—…ë°ì´íŠ¸)")
st.caption("ì—…ë°ì´íŠ¸ ì‹œê°„: " + datetime.now(KST).strftime("%Y-%m-%d %H:%M:%S (KST)"))

def kst_now_iso(): return datetime.now(KST).isoformat()

def load_json(path: str, default: Any):
    try:
        with open(path, "r", encoding="utf-8") as f: return json.load(f)
    except Exception: return default

def to_f(x): 
    try: return None if x is None else float(x)
    except: return None

# ---- ì§€ìˆ˜/í™˜ìœ¨/ì›ìžìž¬: yfinance 1ì°¨ + Stooq(ë°ì´í„°ë¦¬ë”) 2ì°¨ í´ë°± ----
import yfinance as yf
from pandas_datareader import data as pdr

FALLBACK = {
    "KOSPI":  ["^KS11"],
    "KOSDAQ": ["^KQ11","^KOSDAQ","KQ11"],
    "USDKRW": ["KRW=X"],
    "WTI":    ["CL=F"],
    "Gold":   ["GC=F"],
    "Copper": ["HG=F"],
}

def _yf_last2(tick: str)->Tuple[Optional[float],Optional[float]]:
    try:
        df = yf.download(tick, period="10d", interval="1d", progress=False)
        c = df.get("Close")
        if c is None: return None, None
        vals = c.dropna().tail(2).tolist()
        if len(vals)==1: return float(vals[0]), None
        if len(vals)>=2: return float(vals[-1]), float(vals[-2])
    except: pass
    return None, None

def _stooq_last2(symbol: str)->Tuple[Optional[float],Optional[float]]:
    # Stooq ì‹¬ë³¼ì€ ì „ì„¸ê³„ ê³µí†µì´ì§€ë§Œ KOSPI/KOSDAQì€ ì—†ì„ ìˆ˜ ìžˆìŒ(ê·¸ ê²½ìš° None ë°˜í™˜)
    try:
        df = pdr.DataReader(symbol, "stooq")  # ì˜ˆ: ^DJI, ^SPX ë“± â€” êµ­ë‚´ëŠ” ë¯¸ì§€ì›ì¼ ìˆ˜ë„
        c = df.get("Close")
        if c is None: return None, None
        vals = c.sort_index().dropna().tail(2).tolist()
        if len(vals)==1: return float(vals[0]), None
        if len(vals)>=2: return float(vals[-1]), float(vals[-2])
    except: pass
    return None, None

def last2_any(candidates: List[str])->Tuple[Optional[float],Optional[float],Optional[str]]:
    # 1) yfinance ì‹œë„
    for t in candidates:
        cur, prev = _yf_last2(t); 
        if cur is not None: return cur, prev, t
    # 2) stooq ì‹œë„
    for t in candidates:
        cur, prev = _stooq_last2(t); 
        if cur is not None: return cur, prev, f"stooq:{t}"
    return None, None, None

def pct(cur, prev):
    try:
        if cur is None or prev in (None, 0): return None
        return round((cur-prev)/prev*100,2)
    except: return None

def load_market()->Dict[str,Any]:
    data = load_json("data/market_today.json", {})
    updated=False
    for name, cands in FALLBACK.items():
        cur = to_f(data.get(name,{}).get("value"))
        asof = data.get(name,{}).get("asof")
        stale = True
        try:
            if asof: stale = (datetime.now(KST)-datetime.fromisoformat(asof)).total_seconds()>6*3600
        except: pass
        if cur is None or stale:
            v, p, used = last2_any(cands)
            data[name] = {
                "value": None if v is None else round(v,2),
                "prev": None if p is None else round(p,2),
                "change_pct": pct(v,p),
                "ticker": used,
                "asof": kst_now_iso()
            }
            updated=True
    if updated:
        try:
            os.makedirs("data", exist_ok=True)
            with open("data/market_today.json","w",encoding="utf-8") as f:
                json.dump(data,f,ensure_ascii=False,indent=2)
        except: pass
    return data

# ---- ë‰´ìŠ¤: íŒŒì¼ ì—†ìœ¼ë©´ RSS ì§ì ‘ ì½ì–´ì„œ ìƒì„± (Google ë‰´ìŠ¤/ì •ì±…ë¸Œë¦¬í•‘) ----
import feedparser

NEWS_QUERIES = [
    # ê²½ì œ/ì •ì±…/ì‚°ì—…/ë¦¬í¬íŠ¸
    "site:mk.co.kr ê²½ì œ", "site:hankyung.com ê²½ì œ", "site:biz.chosun.com ì‚°ì—…",
    "site:news1.kr ì •ì±…", "site:yna.co.kr ë¦¬í¬íŠ¸", "site:policy.go.kr ì •ì±…ë¸Œë¦¬í•‘"
]

def google_rss(query, hl="ko", gl="KR", ceid="KR:ko"):
    # ê³µë°±/ORë¥¼ URLì— ì•ˆì „í•˜ê²Œ
    from urllib.parse import quote
    return f"https://news.google.com/rss/search?q={quote(query)}&hl={hl}&gl={gl}&ceid={ceid}"

def fetch_headlines_top10()->List[Dict[str,str]]:
    items=[]
    for q in NEWS_QUERIES:
        url=google_rss(q)
        try:
            feed = feedparser.parse(url)
            for e in feed.entries[:5]:
                title = e.get("title","").strip()
                link  = e.get("link","").strip()
                if title and link:
                    items.append({"title":title,"link":link})
        except: 
            pass
    # ì¤‘ë³µ ì œê±°
    seen=set(); uniq=[]
    for it in items:
        if it["title"] in seen: continue
        seen.add(it["title"]); uniq.append(it)
    return uniq[:10]

# ---- í…Œë§ˆ ì¶”ì¶œ: í‚¤ì›Œë“œ ë§¤í•‘ ê¸°ë°˜ ìžë™ ì§‘ê³„ ----
THEME_KEYWORDS = {
    "AI": ["AI","ì¸ê³µì§€ëŠ¥","ìƒì„±í˜•","ì±—GPT","LLM"],
    "ë°˜ë„ì²´": ["ë°˜ë„ì²´","HBM","íŒŒìš´ë“œë¦¬","ë©”ëª¨ë¦¬","GPU","ì¹©"],
    "ë¡œë´‡": ["ë¡œë´‡","í˜‘ë™ë¡œë´‡","ìžìœ¨ì£¼í–‰ë¡œë´‡"],
    "ì´ì°¨ì „ì§€": ["ì´ì°¨ì „ì§€","ë°°í„°ë¦¬","ì–‘ê·¹ìž¬","ìŒê·¹ìž¬","ì „ê³ ì²´"],
    "ë°”ì´ì˜¤": ["ë°”ì´ì˜¤","ì œì•½","ì˜ì•½í’ˆ","ìž„ìƒ"],
    "ì¡°ì„ ": ["ì¡°ì„ ","ì„ ë°•","í•´ìš´","LNGì„ "],
    "ì›ì „": ["ì›ì „","SMR","ì›ìžë ¥"],
    "ì—ë„ˆì§€": ["ì „ë ¥","ì •ìœ ","ê°€ìŠ¤","ìž¬ìƒì—ë„ˆì§€","í’ë ¥","íƒœì–‘ê´‘"],
}

# ëŒ€í‘œ ì¢…ëª©(ì˜µì…˜ í‘œê¸°)
THEME_STOCKS = {
    "AI": ["ì‚¼ì„±ì „ìž","ë„¤ì´ë²„","ì¹´ì¹´ì˜¤","ë”ì¡´ë¹„ì¦ˆì˜¨","í‹°ë§¥ìŠ¤ì†Œí”„íŠ¸"],
    "ë°˜ë„ì²´": ["ì‚¼ì„±ì „ìž","SKí•˜ì´ë‹‰ìŠ¤","DBí•˜ì´í…","í•œë¯¸ë°˜ë„ì²´","í…ŒìŠ¤"],
    "ë¡œë´‡": ["ë ˆì¸ë³´ìš°ë¡œë³´í‹±ìŠ¤","ìœ ì§„ë¡œë´‡","í‹°ë¡œë³´í‹±ìŠ¤","ë¡œë³´ìŠ¤íƒ€","í˜„ëŒ€ë¡œë³´í‹±ìŠ¤"],
    "ì´ì°¨ì „ì§€": ["LGì—ë„ˆì§€ì†”ë£¨ì…˜","í¬ìŠ¤ì½”í“¨ì²˜ì— ","ì—ì½”í”„ë¡œ","ì—ì½”í”„ë¡œë¹„ì— ","ì—˜ì•¤ì—í”„"],
    "ë°”ì´ì˜¤": ["ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤","ì…€íŠ¸ë¦¬ì˜¨","HLB","ì—ìŠ¤í‹°íŒœ","ë©”ë””í†¡ìŠ¤"],
    "ì¡°ì„ ": ["HDí•œêµ­ì¡°ì„ í•´ì–‘","HDí˜„ëŒ€ë¯¸í¬","ì‚¼ì„±ì¤‘ê³µì—…","í•œí™”ì˜¤ì…˜","HSDì—”ì§„"],
    "ì›ì „": ["ë‘ì‚°ì—ë„ˆë¹Œë¦¬í‹°","ìš°ì§„","í•œì „KPS","í•œì „ê¸°ìˆ ","ì¼ì§„íŒŒì›Œ"],
    "ì—ë„ˆì§€": ["í•œêµ­ì „ë ¥","ë‘ì‚°ì—ë„ˆë¹Œë¦¬í‹°","GS","SKì´ë…¸ë² ì´ì…˜","í•œêµ­ê°€ìŠ¤ê³µì‚¬"],
}

def tokenize_ko(text:str)->List[str]:
    # ê°„ë‹¨ í† í¬ë‚˜ì´ì €(í˜•íƒœì†Œê¸°ë°˜ ì•„ë‹˜) â€” í•œê¸€/ì˜ë¬¸/ìˆ«ìžë§Œ ë‚¨ê¹€
    text = re.sub(r"[^0-9A-Za-zê°€-íž£ ]"," ", text)
    return [t for t in text.split() if t]

def score_themes(news: List[Dict[str,str]])->pd.DataFrame:
    counts = {k:0 for k in THEME_KEYWORDS}
    sample = {k:"" for k in THEME_KEYWORDS}
    for n in news:
        title = n.get("title","")
        tokens = tokenize_ko(title)
        tset = " ".join(tokens)
        for theme, keys in THEME_KEYWORDS.items():
            if any(k in tset for k in keys):
                counts[theme]+=1
                if not sample[theme]: sample[theme]=n.get("link","#")
    rows=[]
    for th, ct in counts.items():
        if ct>0:
            rows.append({
                "theme": th,
                "count": ct,
                "score": ct,      # ê°„ë‹¨ ì ìˆ˜ = ë¹ˆë„ (ì›í•˜ë©´ ê°ì‡  ê°€ì¤‘ì¹˜ ì¶”ê°€ ê°€ëŠ¥)
                "rep_stocks": " Â· ".join(THEME_STOCKS.get(th, [])),
                "sample_link": sample[th]
            })
    return pd.DataFrame(rows).sort_values(["score","count"], ascending=False)

def monthly_keywords(news: List[Dict[str,str]])->pd.DataFrame:
    bag={}
    for n in news:
        for w in tokenize_ko(n.get("title","")):
            if len(w)<2: continue
            bag[w]=bag.get(w,0)+1
    rows = sorted(bag.items(), key=lambda x:x[1], reverse=True)[:30]
    return pd.DataFrame([{"keyword":k,"count":v} for k,v in rows])

# ---- ë°ì´í„° ì¤€ë¹„ (íŒŒì¼ ì—†ìœ¼ë©´ ì¦‰ì„ ìƒì„±) ----
market = load_market()

headlines = load_json("data/headlines_top10.json", [])
if not headlines:
    headlines = fetch_headlines_top10()
    try:
        os.makedirs("data", exist_ok=True)
        with open("data/headlines_top10.json","w",encoding="utf-8") as f:
            json.dump(headlines,f,ensure_ascii=False,indent=2)
    except: pass

themes_df = None
try:
    raw_themes = load_json("data/themes_scored.json", [])
    if isinstance(raw_themes, list) and raw_themes:
        # íŒŒì¼ì´ ìžˆìœ¼ë©´ ê·¸ê±¸ ì‚¬ìš©
        themes_df = pd.DataFrame(raw_themes)
except: pass

if themes_df is None:
    # íŒŒì¼ì´ ì—†ìœ¼ë©´ í—¤ë“œë¼ì¸ 100ê°œê¹Œì§€ ì¦‰ì„ ìˆ˜ì§‘ í›„ í…Œë§ˆ ì§‘ê³„
    more_news = headlines[:]
    if len(more_news) < 60:
        # ì¿¼ë¦¬ë³„ ë” ê°€ì ¸ì™€ì„œ ì±„ìš°ê¸°
        for q in NEWS_QUERIES:
            url = google_rss(q)
            try:
                feed = feedparser.parse(url)
                for e in feed.entries[:20]:
                    t = e.get("title","").strip()
                    l = e.get("link","").strip()
                    if t and l: more_news.append({"title":t,"link":l})
            except: pass
    themes_df = score_themes(more_news)

keywords_df = None
try:
    raw_kw = load_json("data/keywords_monthly.json", [])
    if isinstance(raw_kw, list) and raw_kw:
        keywords_df = pd.DataFrame(raw_kw)
except: pass
if keywords_df is None:
    keywords_df = monthly_keywords(headlines)

# ---- ë Œë”ë§ ----
def metric_block(col, title, entry: Dict[str,Any]):
    v = to_f(entry.get("value"))
    d = to_f(entry.get("change_pct"))
    if v is None: col.metric(title, value="-", delta="None")
    else: col.metric(title, value=f"{v:,.2f}", delta=("None" if d is None else f"{d:+.2f}%"))

st.subheader("ðŸ“Š ì˜¤ëŠ˜ì˜ ì‹œìž¥ ìš”ì•½")
c1,c2,c3 = st.columns(3); c4,c5,c6 = st.columns(3)
metric_block(c1, "KOSPI"        , market.get("KOSPI",{}))
metric_block(c2, "KOSDAQ"       , market.get("KOSDAQ",{}))
metric_block(c3, "í™˜ìœ¨(USD/KRW)" , market.get("USDKRW",{}))
metric_block(c4, "WTI"          , market.get("WTI",{}))
metric_block(c5, "Gold"         , market.get("Gold",{}))
metric_block(c6, "Copper"       , market.get("Copper",{}))

st.divider()
st.subheader("ðŸ“° ìµœì‹  ê²½ì œÂ·ì •ì±…Â·ì‚°ì—…Â·ë¦¬í¬íŠ¸ ë‰´ìŠ¤ TOP 10")
if not headlines:
    st.info("í—¤ë“œë¼ì¸ ì—†ìŒ")
else:
    for i,n in enumerate(headlines[:10], start=1):
        title = n.get("title","").replace("[","ï¼»").replace("]","ï¼½")
        link  = n.get("link","#") or "#"
        st.markdown(f"{i}. [{title}]({link})")

st.divider()
st.subheader("ðŸ”¥ ë‰´ìŠ¤ ê¸°ë°˜ TOP í…Œë§ˆ")
if themes_df is None or themes_df.empty:
    st.info("í…Œë§ˆ ë°ì´í„° ì—†ìŒ")
else:
    st.bar_chart(themes_df.set_index("theme")["count"])
    with st.expander("ì „ì²´ í…Œë§ˆ ì§‘ê³„ (ëŒ€í‘œ ì¢…ëª©/ìƒ˜í”Œë§í¬ í¬í•¨)"):
        st.dataframe(themes_df.reset_index(drop=True), use_container_width=True)

st.divider()
st.subheader("ðŸŒ ì›”ê°„ í‚¤ì›Œë“œë§µ (ìµœê·¼ 30ì¼)")
if keywords_df is None or keywords_df.empty:
    st.info("í‚¤ì›Œë“œ ì—†ìŒ")
else:
    st.bar_chart(keywords_df.set_index("keyword")["count"])

st.success("ëŒ€ì‹œë³´ë“œ ë¡œë”© ì™„ë£Œ (ìžê¸‰ìžì¡± ëª¨ë“œ + í´ë°±/ë°©ì–´ ì ìš©)")
