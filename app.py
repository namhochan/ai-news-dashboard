# -*- coding: utf-8 -*-
import math, re
import numpy as np
import pandas as pd
from datetime import datetime, timedelta, date
from zoneinfo import ZoneInfo
from urllib.parse import quote_plus
from collections import Counter

import streamlit as st
import yfinance as yf
import feedparser
from bs4 import BeautifulSoup
from sklearn.linear_model import LogisticRegression

KST = ZoneInfo("Asia/Seoul")

# =========================================
# ğŸ§  ê¸°ë³¸ ì„¤ì •
# =========================================
st.set_page_config(
    page_title="AI ë‰´ìŠ¤ë¦¬í¬íŠ¸ â€“ ìë™ í…Œë§ˆÂ·ì‹œì„¸ ì˜ˆì¸¡",
    layout="wide",
)

# ìˆ«ì í¬ë§·
def fmt_number(v, d=2):
    try:
        if v is None or math.isnan(v): return "-"
        return f"{v:,.{d}f}"
    except Exception:
        return "-"

def fmt_percent(v):
    try:
        if v is None or math.isnan(v): return "-"
        return f"{v:+.2f}%"
    except Exception:
        return "-"

# =========================================
# ğŸ“ˆ ì‹œì„¸ ìˆ˜ì§‘
# =========================================
def fetch_quote(ticker: str):
    try:
        t = yf.Ticker(ticker)
        last, prev = getattr(t.fast_info, "last_price", None), getattr(t.fast_info, "previous_close", None)
        if last and prev:
            return float(last), float(prev)
    except Exception:
        pass
    try:
        df = yf.download(ticker, period="7d", interval="1d", progress=False)
        c = df["Close"].dropna()
        if len(c) == 0: return None, None
        return float(c.iloc[-1]), float(c.iloc[-2]) if len(c) > 1 else None
    except Exception:
        return None, None

# =========================================
# ğŸ“° ë‰´ìŠ¤ ìˆ˜ì§‘ (Google RSS)
# =========================================
def clean_html(raw): return BeautifulSoup(raw or "", "html.parser").get_text(" ", strip=True)

def _parse_entries(feed, days):
    now = datetime.now(KST)
    out = []
    for e in feed.entries:
        t = None
        if getattr(e, "published_parsed", None):
            t = datetime(*e.published_parsed[:6], tzinfo=KST)
        if t and (now - t).days > days: continue
        title, link = e.get("title", ""), e.get("link", "")
        if link.startswith("./"): link = "https://news.google.com/" + link[2:]
        desc = clean_html(e.get("summary", ""))
        out.append({"title": title, "link": link, "time": t.strftime("%Y-%m-%d %H:%M") if t else "-", "desc": desc})
    return out

def fetch_google_news_by_keyword(keyword, days=3, limit=40):
    q = quote_plus(keyword)
    url = f"https://news.google.com/rss/search?q={q}&hl=ko&gl=KR&ceid=KR%3Ako"
    feed = feedparser.parse(url, request_headers={"User-Agent": "Mozilla/5.0"})
    return _parse_entries(feed, days)[:limit]

CATEGORIES = {
    "ê²½ì œë‰´ìŠ¤": ["ê²½ì œ","ê¸ˆë¦¬","ë¬¼ê°€","í™˜ìœ¨","ì„±ì¥ë¥ ","ë¬´ì—­"],
    "ì£¼ì‹ë‰´ìŠ¤": ["ì½”ìŠ¤í”¼","ì½”ìŠ¤ë‹¥","ì¦ì‹œ","ì£¼ê°€","ì™¸êµ­ì¸ ë§¤ìˆ˜","ê¸°ê´€ ë§¤ë„"],
    "ì‚°ì—…ë‰´ìŠ¤": ["ë°˜ë„ì²´","AI","ë°°í„°ë¦¬","ìë™ì°¨","ë¡œë´‡","ìˆ˜ì¶œì…"],
    "ì •ì±…ë‰´ìŠ¤": ["ì •ì±…","ì •ë¶€","ì˜ˆì‚°","ê·œì œ","ì„¸ê¸ˆ","ì‚°ì—…ë¶€"],
}

def fetch_category_news(cat, days=3, max_items=100):
    seen=set(); out=[]
    for kw in CATEGORIES.get(cat, []):
        for it in fetch_google_news_by_keyword(kw, days):
            k=(it["title"],it["link"])
            if k in seen: continue
            seen.add(k); out.append(it)
    def key(x):
        try: return datetime.strptime(x["time"],"%Y-%m-%d %H:%M")
        except: return datetime.min
    return sorted(out,key=key,reverse=True)[:max_items]

# =========================================
# ğŸ’¹ ì‹¤ì‹œê°„ ì§€ìˆ˜ í‹°ì»¤ë°”
# =========================================
def build_ticker_items():
    rows=[("KOSPI","^KS11",2),("KOSDAQ","^KQ11",2),
          ("DOW","^DJI",2),("NASDAQ","^IXIC",2),
          ("USD/KRW","KRW=X",2),("WTI","CL=F",2),
          ("Gold","GC=F",2),("Copper","HG=F",3)]
    items=[]
    for name,ticker,dp in rows:
        last,prev=fetch_quote(ticker)
        d,p=None,None
        if last and prev:
            d=last-prev; p=(d/prev)*100
        items.append({"name":name,"last":fmt_number(last,dp),
                      "pct":fmt_percent(p),"is_up":(d or 0)>0,"is_down":(d or 0)<0})
    return items

TICKER_CSS = """
<style>
.ticker-wrap{overflow:hidden;width:100%;border:1px solid #263042;border-radius:10px;background:#0f1420;}
.ticker-track{display:flex;gap:16px;align-items:center;width:max-content;
  will-change:transform;animation:ticker-scroll var(--speed,30s) linear infinite;}
@keyframes ticker-scroll{0%{transform:translateX(0);}100%{transform:translateX(-50%);}}
.badge{display:inline-flex;align-items:center;gap:8px;background:#0f1420;
  border:1px solid #2b3a55;color:#c7d2fe;padding:6px 10px;border-radius:8px;font-weight:700;white-space:nowrap;}
.badge .name{color:#9fb3c8;font-weight:600;}
.badge .up{color:#e66;} .badge .down{color:#6aa2ff;} .sep{color:#44526b;padding:0 6px;}
</style>
"""
st.markdown(TICKER_CSS, unsafe_allow_html=True)

def render_ticker_line(items,speed_sec=30):
    chips=[]
    for it in items:
        arrow="â–²" if it["is_up"] else ("â–¼" if it["is_down"] else "â€¢")
        cls="up" if it["is_up"] else ("down" if it["is_down"] else "")
        chips.append(f"<span class='badge'><span class='name'>{it['name']}</span>{it['last']} <span class='{cls}'>{arrow} {it['pct']}</span></span>")
    line='<span class="sep">|</span>'.join(chips)
    st.markdown(f"<div class='ticker-wrap' style='--speed:{speed_sec}s'><div class='ticker-track'>{line}<span class='sep'>|</span>{line}</div></div>",unsafe_allow_html=True)

st.markdown("## ğŸ§  AI ë‰´ìŠ¤ë¦¬í¬íŠ¸ â€“ ì‹¤ì‹œê°„ ì§€ìˆ˜ í‹°ì»¤ë°”")
col1,col2=st.columns([1,5])
with col1: st.markdown("### ğŸ“Š ì˜¤ëŠ˜ì˜ ì‹œì¥ ìš”ì•½")
with col2:
    if st.button("ğŸ”„ ìƒˆë¡œê³ ì¹¨"): st.cache_data.clear(); st.rerun()
render_ticker_line(build_ticker_items())
st.caption("â€» ìƒìŠ¹=ë¹¨ê°•, í•˜ë½=íŒŒë‘ Â· ë°ì´í„°: Yahoo Finance")

# =========================================
# ğŸ“° ìµœì‹  ë‰´ìŠ¤ ìš”ì•½
# =========================================
st.divider()
st.markdown("## ğŸ“° ìµœì‹  ë‰´ìŠ¤ ìš”ì•½")
c1,c2=st.columns([2,1])
with c1: cat=st.selectbox("ğŸ“‚ ì¹´í…Œê³ ë¦¬ ì„ íƒ", list(CATEGORIES))
with c2: page=st.number_input("í˜ì´ì§€",min_value=1,value=1,step=1)
news_all=fetch_category_news(cat,3,100)
page_size=10
news_page=news_all[(page-1)*page_size:page*page_size]
if not news_page: st.info("í‘œì‹œí•  ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
else:
    for i,n in enumerate(news_page,1):
        st.markdown(f"**[{n['title']}]({n['link']})**  \n"
                    f"<span style='color:#9aa0a6;font-size:0.9rem;'>{n['time']}</span><br>"
                    f"<span style='color:#aeb8c5;'>{n['desc']}</span>",unsafe_allow_html=True)
        st.markdown("<hr style='border:0;border-top:1px solid #1f2937'/>",unsafe_allow_html=True)
st.caption(f"ìµœê·¼ 3ì¼ â€¢ {len(news_all)}ê±´ ì¤‘ {cat}")

# =========================================
# ğŸ”¥ ë‰´ìŠ¤ ê¸°ë°˜ í…Œë§ˆ ê°ì§€
# =========================================
st.divider()
st.markdown("## ğŸ”¥ ë‰´ìŠ¤ ê¸°ë°˜ í…Œë§ˆ ìš”ì•½")

THEME_KEYWORDS = {
    "AI":["ai","ì¸ê³µì§€ëŠ¥","ì±—ë´‡","ì—”ë¹„ë””ì•„","ì˜¤í”ˆai","ìƒì„±í˜•"],
    "ë°˜ë„ì²´":["ë°˜ë„ì²´","hbm","ì¹©","ë¨","íŒŒìš´ë“œë¦¬"],
    "ë¡œë´‡":["ë¡œë´‡","ììœ¨ì£¼í–‰","í˜‘ë™ë¡œë´‡","amr"],
    "ì´ì°¨ì „ì§€":["ë°°í„°ë¦¬","ì „ê³ ì²´","ì–‘ê·¹ì¬","ìŒê·¹ì¬","lfg"],
    "ì—ë„ˆì§€":["ì—ë„ˆì§€","ì •ìœ ","ì „ë ¥","íƒœì–‘ê´‘","í’ë ¥","ê°€ìŠ¤"],
    "ì¡°ì„ ":["ì¡°ì„ ","ì„ ë°•","lNGì„ ","í•´ìš´"],
    "LNG":["lng","ê°€ìŠ¤ê³µì‚¬","í„°ë¯¸ë„"],
    "ì›ì „":["ì›ì „","smr","ì›ìë ¥","ìš°ë¼ëŠ„"],
    "ë°”ì´ì˜¤":["ë°”ì´ì˜¤","ì œì•½","ì‹ ì•½","ì„ìƒ"],
}

THEME_STOCKS = {  # í™•ì¥ í’€
    "AI":[("ì‚¼ì„±ì „ì","005930.KS"),("ë„¤ì´ë²„","035420.KS"),("ì¹´ì¹´ì˜¤","035720.KS"),
           ("ì†”íŠ¸ë£©ìŠ¤","304100.KQ"),("ë¸Œë ˆì¸ì¦ˆì»´í¼ë‹ˆ","099390.KQ"),("í•œê¸€ê³¼ì»´í“¨í„°","030520.KS")],
    "ë°˜ë„ì²´":[("SKí•˜ì´ë‹‰ìŠ¤","000660.KS"),("DBí•˜ì´í…","000990.KS"),("ë¦¬ë…¸ê³µì—…","058470.KQ"),
           ("ì›ìµIPS","240810.KQ"),("í‹°ì”¨ì¼€ì´","064760.KQ"),("ì—í”„ì—ìŠ¤í‹°","036810.KQ")],
    "ë¡œë´‡":[("ë ˆì¸ë³´ìš°ë¡œë³´í‹±ìŠ¤","277810.KQ"),("ìœ ì§„ë¡œë´‡","056080.KQ"),("í‹°ë¡œë³´í‹±ìŠ¤","117730.KQ"),
           ("ë¡œë³´ìŠ¤íƒ€","090360.KQ"),("ìŠ¤ë§¥","099440.KQ")],
    "ì´ì°¨ì „ì§€":[("LGì—ë„ˆì§€ì†”ë£¨ì…˜","373220.KS"),("í¬ìŠ¤ì½”í“¨ì²˜ì— ","003670.KS"),
           ("ì—ì½”í”„ë¡œ","086520.KQ"),("ì½”ìŠ¤ëª¨ì‹ ì†Œì¬","005070.KQ"),("ì—˜ì•¤ì—í”„","066970.KQ")],
    "ì—ë„ˆì§€":[("í•œêµ­ì „ë ¥","015760.KS"),("ë‘ì‚°ì—ë„ˆë¹Œë¦¬í‹°","034020.KS"),
           ("GS","078930.KS"),("í•œí™”ì†”ë£¨ì…˜","009830.KS"),("OCIí™€ë”©ìŠ¤","010060.KS")],
    "ì¡°ì„ ":[("HDí•œêµ­ì¡°ì„ í•´ì–‘","009540.KS"),("HDí˜„ëŒ€ë¯¸í¬","010620.KS"),
           ("ì‚¼ì„±ì¤‘ê³µì—…","010140.KS"),("í•œí™”ì˜¤ì…˜","042660.KS")],
    "LNG":[("í•œêµ­ê°€ìŠ¤ê³µì‚¬","036460.KS"),("ì§€ì—ìŠ¤ì´","053050.KQ"),("ëŒ€ì„±ì—ë„ˆì§€","117580.KQ"),("SKê°€ìŠ¤","018670.KS")],
    "ì›ì „":[("ë‘ì‚°ì—ë„ˆë¹Œë¦¬í‹°","034020.KS"),("ìš°ì§„","105840.KQ"),("í•œì „KPS","051600.KS"),("ë³´ì„±íŒŒì›Œí…","006910.KQ")],
    "ë°”ì´ì˜¤":[("ì…€íŠ¸ë¦¬ì˜¨","068270.KS"),("ì—ìŠ¤í‹°íŒœ","237690.KQ"),("ì•Œí…Œì˜¤ì  ","196170.KQ"),("ë©”ë””í†¡ìŠ¤","086900.KQ")],
}

def detect_themes(news):
    counts={t:0 for t in THEME_KEYWORDS}
    for n in news:
        text=(n["title"]+" "+n["desc"]).lower()
        for t,kws in THEME_KEYWORDS.items():
            if any(k in text for k in kws): counts[t]+=1
    rows=[{"theme":t,"count":c} for t,c in counts.items() if c>0]
    return sorted(rows,key=lambda x:x["count"],reverse=True)

all_news=[]
for c in CATEGORIES: all_news+=fetch_category_news(c,3,100)
theme_rows=detect_themes(all_news)
if not theme_rows: st.info("í…Œë§ˆ ì‹ í˜¸ ì—†ìŒ.")
else:
    top5=theme_rows[:5]
    st.markdown("**TOP í…Œë§ˆ:** " + " ".join([f"ğŸŸ¢ {r['theme']}({r['count']})" for r in top5]))
    rng=np.random.default_rng(int(date.today().strftime("%Y%m%d")))

    def safe_yf_price(t):
        try:
            l,p=fetch_quote(t)
            if not l or not p: return None,None,"gray"
            d=(l-p)/p*100; c="red" if d>0 else ("blue" if d<0 else "gray")
            return fmt_number(l,0), fmt_percent(d), c
        except: return None,None,"gray"

    for tr in top5:
        theme=tr["theme"]; pool=THEME_STOCKS.get(theme,[])
        if not pool: continue
        idx=rng.choice(len(pool),size=min(4,len(pool)),replace=False)
        stocks=[pool[i] for i in idx]
        st.write(f"**{theme}**")
        cols=st.columns(len(stocks))
        for col,(name,ticker) in zip(cols,stocks):
            with col:
                px,chg,color=safe_yf_price(ticker)
                arrow="â–²" if color=="red" else ("â–¼" if color=="blue" else "â– ")
                st.markdown(f"<b>{name}</b><br><span style='color:{color}'>{px} {arrow} {chg}</span><br><small>{ticker}</small>",unsafe_allow_html=True)
        st.divider()

# =========================================
# ğŸ§  AI ë‰´ìŠ¤ ìš”ì•½ì—”ì§„ (ë”ë³´ê¸°í˜•)
# =========================================
st.divider()
st.markdown("## ğŸ§  AI ë‰´ìŠ¤ ìš”ì•½ì—”ì§„")
titles=[n["title"] for c in CATEGORIES for n in fetch_category_news(c,3,60)]
words=[]
for t in titles:
    t=re.sub(r"[^ê°€-í£A-Za-z0-9\s]"," ",t)
    words+=[w for w in t.split() if len(w)>=2]
top_kw=[w for w,_ in Counter(words).most_common(10)]
st.markdown("### ğŸ“Œ í•µì‹¬ í‚¤ì›Œë“œ TOP10")
st.write(", ".join(top_kw))
# ê°„ë‹¨ ìš”ì•½ë¬¸
full_text=" ".join(titles)
sentences=re.split(r'[.!?]\s+',full_text)
summary=[s for s in sentences if len(s.strip())>20][:5]
st.markdown("### ğŸ“° í•µì‹¬ ìš”ì•½ë¬¸")
if summary:
    st.markdown(f"**ìš”ì•½:** {summary[0][:150]}...")
    with st.expander("ì „ì²´ ìš”ì•½ë¬¸ ë³´ê¸° ğŸ‘‡"):
        for s in summary: st.markdown(f"- {s.strip()}")

# =====================================
# ğŸ“Š 2ë‹¨ê³„: í…Œë§ˆë³„ ìƒìŠ¹ í™•ë¥  ì˜ˆì¸¡ (AI ë¦¬ìŠ¤í¬ë ˆë²¨ + í…Œë§ˆê°•ë„)
# =====================================
st.divider()
st.markdown("## ğŸ“Š AI ìƒìŠ¹ í™•ë¥  ì˜ˆì¸¡ ë¦¬í¬íŠ¸")

def calc_theme_strength(count, avg_delta):
    """í…Œë§ˆê°•ë„: ë‰´ìŠ¤ë¹ˆë„(0~1) + í‰ê· ë“±ë½(0~1)"""
    freq_score = min(count / 20, 1.0)
    price_score = min(max((avg_delta + 5) / 10, 0), 1.0)
    total = (freq_score * 0.6 + price_score * 0.4) * 5
    return round(total, 1)

def calc_risk_level(avg_delta):
    """AI ë¦¬ìŠ¤í¬ ë ˆë²¨ (1~5, í•˜ë½í­ í´ìˆ˜ë¡ ë†’ìŒ)"""
    if avg_delta >= 3: return 1
    if avg_delta >= 1: return 2
    if avg_delta >= -1: return 3
    if avg_delta >= -3: return 4
    return 5

report_rows = []
for tr in theme_rows[:5]:
    theme = tr["theme"]
    stocks = THEME_STOCKS.get(theme, [])
    deltas = []
    for _, ticker in stocks:
        try:
            last, prev = fetch_quote(ticker)
            if last and prev:
                deltas.append((last - prev) / prev * 100)
        except Exception:
            pass
    avg_delta = np.mean(deltas) if deltas else 0
    theme_strength = calc_theme_strength(tr["count"], avg_delta)
    risk_level = calc_risk_level(avg_delta)
    report_rows.append({
        "í…Œë§ˆ": theme,
        "ë‰´ìŠ¤ë¹ˆë„": tr["count"],
        "í‰ê· ë“±ë½(%)": round(avg_delta, 2),
        "í…Œë§ˆê°•ë„(1~5)": theme_strength,
        "ë¦¬ìŠ¤í¬ë ˆë²¨(1~5)": risk_level,
    })

st.dataframe(report_rows, use_container_width=True, hide_index=True)
st.caption("â€» í…Œë§ˆê°•ë„â†‘ = ë‰´ìŠ¤ + ê°€ê²©ì´ ëª¨ë‘ í™œë°œí•œ ìƒíƒœ / ë¦¬ìŠ¤í¬ë ˆë²¨â†‘ = ë³€ë™ì„±Â·í•˜ë½ ê°€ëŠ¥ì„± ë†’ìŒ")
